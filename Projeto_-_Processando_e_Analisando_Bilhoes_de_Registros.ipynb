{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7663fd3f",
   "metadata": {},
   "source": [
    "# <center><span style=\"font-size: 42px;color: darkgreen;\">Projeto - Processando e Analisando Bilhões de Registros com Presto, Hive e AWS EMR (Elastic MapReduce)</center></span>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Contexto\n",
    "\n",
    "<br>\n",
    "\n",
    "No dia a dia, as empresas buscam **processar e analisar dados na maior velocidade possível**. Para atender a essa necessidade, este projeto não utiliza um `pseudo cluster`, mas sim um `multi-node cluster` (um cluster de várias máquinas).\n",
    "\n",
    "Existem diversas alternativas para construir um cluster desse tipo. Aqui, optamos por um **Ambiente em Nuvem**, que, com apenas alguns cliques, permite configurar um `multi-node cluster` capaz de processar grandes volumes de dados. Além disso, não será necessário realizar nenhuma instalação local.\n",
    "\n",
    "O **projeto** consiste em:\n",
    "\n",
    "- **1.** Montar um ambiente de processamento de dados robusto na nuvem.\n",
    "- **2.** Processar e analisar grandes quantidades de dados utilizando ferramentas distribuídas como `Presto`, `Hive` e `AWS Elastic MapReduce (EMR)`.\n",
    "- **3.** Desligar o ambiente após o término do processamento, otimizando custos e recursos.\n",
    "\n",
    "Esse fluxo proporciona eficiência e escalabilidade, atendendo às demandas das empresas modernas de Big Data.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "## Instruções Gerais Sobre o Projeto\n",
    "\n",
    "<br>\n",
    "\n",
    "Este projeto utiliza uma **amostra de dados** composta por **aproximadamente 6,5 milhões de registros**, extraídos de um conjunto maior contendo **2,8 bilhões de registros**.\n",
    "\n",
    "Para processar o conjunto de dados completo, será necessário dispor de **cerca de 300 GB de espaço em disco**. O procedimento para análise será idêntico, independentemente do tamanho do conjunto de dados.\n",
    "\n",
    "A amostra foi selecionada para **facilitar a execução em ambientes com recursos limitados**, mantendo a escalabilidade para processar o dataset completo quando os recursos necessários estiverem disponíveis.\n",
    "\n",
    "Portanto, o projeto será preparado pensando em processar e analisar bilhões de registros, mas, para fins de teste e validação, utilizaremos a amostra de 6,5 milhões de registros. Isso garante eficiência no desenvolvimento enquanto mantém a flexibilidade para lidar com o volume total de dados.\n",
    "\n",
    "> É necessário criar uma conta gratuita na **AWS**.\n",
    "\n",
    "Será necessário realizar **acesso remoto ao servidor AWS**. Certifique-se que nenhum firewall ou proxy na máquina ou rede tenha **restrições de acesso remoto**.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Importante\n",
    "\n",
    "Reproduzir este projeto terá um <span style=\"font-size: 18px;color: red;\">custo associado</span>, embora seja baixo.\n",
    "\n",
    "> Lembre-se de finalizar todos os serviços AWS após a conclusão do Projeto.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Explorando a AWS\n",
    "\n",
    "<br>\n",
    "\n",
    "Após **criar uma conta**, vamos explorar a líder mundial em **Computação em Nuvem**.\n",
    "\n",
    "Ao **efetuar login**, você será direcionado à **Página Inicial do Console de Serviços**. Clique em **\"Ver todos os serviços\"** para explorar a vasta gama de recursos que a AWS oferece.\n",
    "\n",
    "Um dos **serviços** disponíveis na seção **Análise de Dados** é o **EMR** (Elastic MapReduce), que será utilizado neste projeto.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre a Fonte de Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "No site abaixo, você encontrará uma série de arquivos CSV com **dados de corridas de táxi** na cidade de Nova York. Os dados abrangem o período de 2009 a 2024. Para este projeto, trabalharemos com uma **amostra de 6,5 milhões de registros**.\n",
    "\n",
    "- [TLC Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Importante:** O procedimento/configuração é o mesmo, independentemente de estarmos trabalhando com 6,5 milhões ou bilhões de registros.\n",
    "\n",
    "<br>\n",
    "\n",
    "Para este projeto, utilizaremos os dados de 2020. Após acessar o link, clique em `2020`, e em seguida, no arquivo CSV **`Yellow Taxi Trip Records`** do mês de **janeiro**.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Sobre o Conjunto de Dados\n",
    "\n",
    "<br>\n",
    "\n",
    "O conjunto de dados contém informações sobre **corridas de táxi** em Nova York no ano de **2020**, com aproximadamente **6,5 milhões de registros**. O arquivo contém **19 colunas**, incluindo informações como:\n",
    "\n",
    "- **Data e hora de coleta e entrega** das corridas (`tpep_pickup_datetime`, `tpep_dropoff_datetime`).\n",
    "- **Distância da viagem** e **código da tarifa** (`trip_distance`, `RatecodeID`).\n",
    "- **Detalhes sobre o pagamento**, como o valor da tarifa e gorjeta (`fare_amount`, `tip_amount`).\n",
    "- **Localizações de coleta e entrega** (`PULocationID`, `DOLocationID`).\n",
    "- **Informações adicionais** como taxas de pedágio e taxa de melhoria (`tolls_amount`, `improvement_surcharge`).\n",
    "\n",
    "Todo o processo de análise será realizado diretamente na **AWS**, utilizando a infraestrutura em nuvem. O Jupyter será utilizado apenas para escrever o passo a passo do procedimento, e não para a execução dos cálculos. A amostra de **6,5 milhões de registros** será utilizada como base para a análise, representando uma fração significativa do conjunto total.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "### Resumo\n",
    "\n",
    "<br>\n",
    "\n",
    "Este é um projeto de demonstração para que você veja como configurar um **cluster multi-node** e processar grandes volumes de dados. Usaremos o **cluster Hadoop** com **Amazon EMR – Elastic MapReduce** e os serviços de motor de banco de dados **Presto** e **Hive** para análise dos dados.\n",
    "\n",
    "Tudo será demonstrado passo a passo, e o ambiente será configurado nas aulas sem custo na **AWS**. Acompanhe atentamente as instruções que serão passadas durante as aulas.\n",
    "\n",
    "<br><br><br><br><br>\n",
    "\n",
    "---\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# <center><span style=\"font-size: 38px;color: darkgreen;\">Iniciando o Laboratório</center></span>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# 1. Movendo os Dados Para o AWS S3\n",
    "\n",
    "<br>\n",
    "\n",
    "- **1.1** Na **Página Inicial** da AWS, clique em **\"Ver todos os serviços\"**.  \n",
    "- **1.2** Na seção de **Armazenamento**, clique em **S3**.\n",
    "- **1.3** Após acessar o S3, será necessário criar um **Bucket** (um repositório de arquivos):\n",
    "   - O nome do **Bucket** deve ser **único** em toda a AWS, ou seja, deve ser um nome que nunca tenha sido utilizado por nenhuma outra conta.\n",
    "   - O nome escolhido para este projeto será: **`projeto-processando-e-analisando-bilhoes-de-registros-taxis-ny`**.\n",
    "   - Deixe todas as **configurações padrão** e clique em **Criar Bucket**.\n",
    "\n",
    "- **1.4** Após a criação do Bucket, clique sobre o **Bucket** recém-criado. Na próxima página, clique em **Carregar** para começar o processo de upload dos dados.\n",
    "\n",
    "- **1.5** Na página de carregamento, **arraste e solte** o arquivo `yellow_tripdata_2020-01.parquet` para o campo indicado, e clique em **Carregar**.\n",
    "\n",
    "- **1.6** Aguarde o tempo necessário para que o arquivo seja carregado na nuvem.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "# 2. Criando um Multi Node Cluster com EMR (Amazon Elastic MapReduce)\n",
    "\n",
    "<br>\n",
    "\n",
    "Agora, vamos trabalhar com outro serviço da AWS: o **EMR (Elastic MapReduce)**. O EMR é uma plataforma que permite processar grandes volumes de dados usando frameworks como **Apache Hadoop** e **Apache Spark**.\n",
    "\n",
    "Neste passo, faremos com que o **S3** e o **EMR** se comuniquem, permitindo que o EMR acesse os dados armazenados no S3 para processamento distribuído.\n",
    "\n",
    "#### Passos para a Integração\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59822e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af67c970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24687aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71ce4076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6405008 entries, 0 to 6405007\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int64         \n",
      " 1   tpep_pickup_datetime   datetime64[ns]\n",
      " 2   tpep_dropoff_datetime  datetime64[ns]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int64         \n",
      " 8   DOLocationID           int64         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  airport_fee            object        \n",
      "dtypes: datetime64[ns](2), float64(11), int64(4), object(2)\n",
      "memory usage: 928.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad3cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60f966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e9ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b0abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbc329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740fedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb943e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db26c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890b1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bff36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TLC Trip Record Data: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "Analisando 1,1 Bilhão de Viagens de Táxi e Uber de NY (com um toque de vingança): https://toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance\n",
    "\n",
    "New York City Taxi and For-Hire Vehicle Data (GitHub): https://github.com/toddwschneider/nyc-taxi-data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
